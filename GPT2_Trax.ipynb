{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTjPr/nuT4hmLcn9+kR63s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lastget/GPT2_TRAX/blob/main/GPT2_Trax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2 transformer decoder\n",
        "Built with Trax.  "
      ],
      "metadata": {
        "id": "gUidpeYxmA8Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfrSTqzUl8qw",
        "outputId": "58566d56-6c71-4e7f-bcd7-f22269518c24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m637.9/637.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q trax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import gin\n",
        "\n",
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=70)\n",
        "\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax.fastmath import numpy as jnp\n",
        "\n",
        "# to print the entire np array\n",
        "np.set_printoptions(threshold=sys.maxsize)"
      ],
      "metadata": {
        "id": "ACM895MZnmt4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Position Encoding"
      ],
      "metadata": {
        "id": "UN2PeBpYntEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\n",
        "    \"\"\"Returns a list of layers that:\n",
        "    1. takes a block of text as input,\n",
        "    2. embeds the words in that text, and\n",
        "    3. adds positional encoding,\n",
        "       i.e. associates a number in range(max_len) with\n",
        "       each word in each sentence of embedded input text\n",
        "\n",
        "    The input is a list of tokenized blocks of text\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): vocab size.\n",
        "        d_model (int):  depth of embedding.\n",
        "        dropout (float): dropout rate (how much to drop out).\n",
        "        max_len (int): maximum symbol length for positional encoding.\n",
        "        mode (str): 'train' or 'eval'.\n",
        "    \"\"\"\n",
        "    # Embedding inputs and positional encoder\n",
        "    return [\n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\n",
        "        tl.Embedding(vocab_size, d_model),\n",
        "        # Use dropout with rate and mode specified\n",
        "        tl.Dropout(rate=dropout, mode=mode),\n",
        "        # Add positional encoding layer with maximum input length and mode specified\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycNEVqjNnqMj",
        "outputId": "1715e7fe-83a7-4b93-c1be-81a8b43ca206"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feed-forward layer"
      ],
      "metadata": {
        "id": "bp66H69BoL_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def FeedForward(d_model, d_ff, dropout, mode, ff_activation):\n",
        "    \"\"\"Returns a list of layers that implements a feed-forward block.\n",
        "\n",
        "    The input is an activation tensor.\n",
        "\n",
        "    Args:\n",
        "        d_model (int):  depth of embedding.\n",
        "        d_ff (int): depth of feed-forward layer.\n",
        "        dropout (float): dropout rate (how much to drop out).\n",
        "        mode (str): 'train' or 'eval'.\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\n",
        "\n",
        "    Returns:\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create feed-forward block (list) with two dense layers with dropout and input normalized\n",
        "    return [\n",
        "        # Normalize layer inputs\n",
        "        tl.LayerNorm(),\n",
        "        # Add first feed forward (dense) layer (don't forget to set the correct value for n_units)\n",
        "        tl.Dense(d_ff),\n",
        "        # Add activation function passed in as a parameter (you need to call it!)\n",
        "        ff_activation(),  # Generally ReLU\n",
        "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
        "        tl.Dropout(rate=dropout, mode=mode),\n",
        "        # Add second feed forward layer (don't forget to set the correct value for n_units)\n",
        "        tl.Dense(d_model),\n",
        "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
        "        tl.Dropout(rate=dropout, mode=mode)\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "Z-RSAoA7n-vL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder Block\n"
      ],
      "metadata": {
        "id": "KRhZBhlpojpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def DecoderBlock(d_model, d_ff, n_heads,\n",
        "                 dropout, mode, ff_activation):\n",
        "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\n",
        "\n",
        "    The input is an activation tensor.\n",
        "\n",
        "    Args:\n",
        "        d_model (int):  depth of embedding.\n",
        "        d_ff (int): depth of feed-forward layer.\n",
        "        n_heads (int): number of attention heads.\n",
        "        dropout (float): dropout rate (how much to drop out).\n",
        "        mode (str): 'train' or 'eval'.\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\n",
        "\n",
        "    Returns:\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    # Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\n",
        "    return [\n",
        "      tl.Residual(\n",
        "          # Normalize layer input\n",
        "          tl.LayerNorm(),\n",
        "          # Add causal attention\n",
        "          tl.CausalAttention(d_model, n_heads=n_heads, dropout=dropout, mode=mode)\n",
        "        ),\n",
        "      tl.Residual(\n",
        "          # Add feed-forward block\n",
        "          # We don't need to normalize the layer inputs here. The feed-forward block takes care of that for us.\n",
        "          FeedForward(d_model, d_ff, dropout, mode, ff_activation)\n",
        "        ),\n",
        "      ]"
      ],
      "metadata": {
        "id": "tOzXhb3SoiNm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Decoder all together"
      ],
      "metadata": {
        "id": "ntecHcmHosT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def TransformerLM(vocab_size=33300,\n",
        "                  d_model=512,\n",
        "                  d_ff=2048,\n",
        "                  n_layers=6,\n",
        "                  n_heads=8,\n",
        "                  dropout=0.1,\n",
        "                  max_len=4096,\n",
        "                  mode='train',\n",
        "                  ff_activation=tl.Relu):\n",
        "    \"\"\"Returns a Transformer language model.\n",
        "\n",
        "    The input to the model is a tensor of tokens. (This model uses only the\n",
        "    decoder part of the overall Transformer.)\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): vocab size.\n",
        "        d_model (int):  depth of embedding.\n",
        "        d_ff (int): depth of feed-forward layer.\n",
        "        n_layers (int): number of decoder layers.\n",
        "        n_heads (int): number of attention heads.\n",
        "        dropout (float): dropout rate (how much to drop out).\n",
        "        max_len (int): maximum symbol length for positional encoding.\n",
        "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\n",
        "\n",
        "    Returns:\n",
        "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\n",
        "        to activations over a vocab set.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create stack (list) of decoder blocks with n_layers with necessary parameters\n",
        "    decoder_blocks = [\n",
        "        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)]\n",
        "\n",
        "    # Create the complete model as written in the figure\n",
        "    return tl.Serial(\n",
        "        # Use teacher forcing (feed output of previous step to current step)\n",
        "        tl.ShiftRight(mode=mode),\n",
        "        # Add embedding inputs and positional encoder\n",
        "        PositionalEncoder(vocab_size, d_model, dropout, max_len, mode),\n",
        "        # Add decoder blocks\n",
        "        decoder_blocks,\n",
        "        # Normalize layer\n",
        "        tl.LayerNorm(),\n",
        "\n",
        "        # Add dense layer of vocab_size (since need to select a word to translate to)\n",
        "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\n",
        "        tl.Dense(vocab_size),\n",
        "        # Get probabilities with Logsoftmax\n",
        "        tl.LogSoftmax()\n",
        "    )"
      ],
      "metadata": {
        "id": "0ll_jq14optZ"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}